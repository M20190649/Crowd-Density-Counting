{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Crowd Estimation Made Easy </center>\n",
    "\n",
    "The problem of <b> crowd </b> is everywhere and it is our one of global problem. The problem of crowd counting is being studied on many research purpose from decades. There are various approaches to find the best way to estimate the number of people on the crowd. There is nothing like magic in the field of AI but we have the algorithms which solves our problems mathematically. The estimation of people on the crowd requires dataset. There are many datasets publicly available on the web. <br/>\n",
    "There has been lots of research on this field since decades and some are still ongoing. We are including some of those research. But here we are including some of research from 2018 here. Follow this [Github Link](https://github.com/gjy3035/Awesome-Crowd-Counting)for more information about crowd counting.\n",
    "\n",
    "## Contents:\n",
    "\n",
    "* [Popular Datasets](#datasets)\n",
    "\n",
    "## Popular Datasets\n",
    "\n",
    "* [INRIA Person datasets](http://pascal.inrialpes.fr/data/human/)\n",
    "* [Mall Dataset](http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html)\n",
    "* [Caltech Pedestrian Dataset](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/datasets/USA/)\n",
    "* ShanghaiTech Dataset ([Dropbox](https://www.dropbox.com/s/fipgjqxl7uj8hd5/ShanghaiTech.zip?dl=0))\n",
    "* [World Expo Dataset](http://www.ee.cuhk.edu.hk/~xgwang/expo.html)\n",
    "* [Grand Central Station Dataset](http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html)\n",
    "* [UCF CC 50](https://www.crcv.ucf.edu/data/ucf-cc-50/)\n",
    "* [UCF-QNRF - A Large Crowd Counting Data Set](https://www.crcv.ucf.edu/data/ucf-qnrf/)\n",
    "\n",
    "\n",
    "\n",
    "### INRIA Person datasets:\n",
    "It includes large set of marked up images of standing or walking people. The dataset is divided in two formats: (a) original images with corresponding annotation files, and (b) positive images in normalized 64x128 pixel format (as used in the CVPR paper) with original negative images. Sample from the dataset is given below:\n",
    "<img src = 'https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/3b304585d5af0afe98a85d6e0559315fbf3a7807/5-Figure2-1.png'>\n",
    "<center><a href = 'https://www.semanticscholar.org/paper/An-Improved-Labelling-for-the-INRIA-Person-Data-Set-Taiana-Nascimento/3b304585d5af0afe98a85d6e0559315fbf3a7807'>Source</a></center>\n",
    "\n",
    "\n",
    "### Mall Datasets:\n",
    "The dataset is composed by RGB images of frames in a video (as inputs) and the object counting on every frame, this is the number of pedestrians (object) in the image. The images are 480x640 pixels at 3 channels of the same spot recorded by a webcam in a mall but it has different number of person on every frame, is a problem of crowd counting. It have below properties:\n",
    "* Video length: 2000 frames\n",
    "* Frame size: 640x480 \n",
    "* Frame rate: < 2 Hz \n",
    "\n",
    "In dataset, over 60,000 pedestrians were labelled in 2000 video frames.\n",
    "<img src = 'http://personal.ie.cuhk.edu.hk/~ccloy/images/shopping_mall.jpg'>\n",
    "<center>Example frame</center>\n",
    "\n",
    "#### Table\n",
    "| Year-Conference/Journal | Method | MAE | MSE |\n",
    "| --- | --- | --- | --- |\n",
    "| 2018--CVPR | [DecideNet](https://arxiv.org/abs/1712.06679)                 | 1.52 | 1.90 |\n",
    "| 2018--IJCAI| [DRSAN](https://arxiv.org/abs/1807.00601)                         | 1.72 | 2.1  |\n",
    "| 2019--WACV | **[SAAN](http://www.cs.umanitoba.ca/~ywang/papers/wacv19.pdf)**                       | **1.28** | **1.68** |\n",
    "\n",
    "\n",
    "\n",
    "### Caltech Pedestrian Datasets:\n",
    "The Caltech Pedestrian Dataset consists of approximately 10 hours of 640x480 30Hz video taken from a vehicle driving through regular traffic in an urban environment. About 250,000 frames (in 137 approximately minute long segments) with a total of 350,000 bounding boxes and 2300 unique pedestrians were annotated. The annotation includes temporal correspondence between bounding boxes and detailed occlusion labels. For more information follow the [link](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/files/PAMI12pedestrians.pdf). \n",
    "<img src = 'http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/files/peds01_web.jpg'>\n",
    "<center>Sample example </center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Shanghai Tech Dataset:\n",
    "These datasets are appeared in CVPR 2016 paper [Single Image Crowd Counting via Multi Column Convolutional Neural Network](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf). It have two different dataset, Par A and Part B. In each dataset , there are 3 folder:\n",
    "* images: the jpg image file\n",
    "* ground-truth: matlab file contain annotated head (coordinate x, y)\n",
    "* ground-truth-h5: people density map\n",
    "<img src = ' '>\n",
    "\n",
    "#### PartA:\n",
    " \n",
    "| Year-Conference/Journal | Methods           | MAE   | MSE   | PSNR  | SSIM | Params | Pre-trained   Model |\n",
    "| ---- | ------------------------------------ | ----- | ----- | ----- | ---- | ------ | ------------------- |\n",
    "| 2018--AAAI | [TDF-CNN](#TDF-CNN)                          | 97.5  | 145.1 | -  | -  | -  | -           |\n",
    "| 2018--WACV | [SaCNN](#SaCNN)                              | 86.8  | 139.2 | -  | -  | -  | -           |\n",
    "| 2018--CVPR | [ACSCP](#ACSCP)                              | 75.7  | 102.7 | -  | -  | 5.1M | None      |\n",
    "| 2018--CVPR | [D-ConvNet-v1](#D-ConvNet)                   | 73.5  | 112.3 | -  | -  | -  | -           |\n",
    "| 2018--CVPR | [IG-CNN](#IG-CNN)                            | 72.5  | 118.2 | -  | -  | -  | -           |\n",
    "| 2018--CVPR | [L2R](#L2R) (Multi-task,   Query-by-example) | 72.0  | 106.6 | -  | -  | -  | VGG-16      |\n",
    "| 2018--CVPR | [L2R](#L2R) (Multi-task,   Keyword)          | 73.6  | 112.0 | -  | -  | -  | VGG-16      |\n",
    "| 2018--IJCAI| [DRSAN](#DRSAN)                              | 69.3  | 96.4  | -  | -  | -  | -           |\n",
    "| 2018--ECCV | [ic-CNN](#ic-CNN) (one stage)                | 69.8  | 117.3 | -  | -  | -  | -           |\n",
    "| 2018--ECCV | [ic-CNN](#ic-CNN) (two stages)               | 68.5  | 116.2 | -  | -  | -  | -           |\n",
    "| 2018--CVPR | [CSRNet](#CSR)   | 68.2  | 115.0 | 23.79 | 0.76 | 16.26M<sup>[SANet](#SANet)</sup> |VGG-16|\n",
    "| 2018--ECCV | [SANet](#SANet)                              | 67.0  | 104.5 | -  | -  | 0.91M | None     |\n",
    "| 2019--AAAI | [GWTA-CCNN](#GWTA-CCNN)                      | 154.7 | 229.4 | -  | -  | -  | -           |\n",
    "| 2019--ICASSP | [ASD](#ASD)                                | 65.6  | 98.0  | -  | -  | -  | -           |\n",
    "| 2019--ICCV | [CFF](#CFF)                                  | 65.2  | 109.4 | 25.4  | 0.78 | -     | -   |\n",
    "| 2019--CVPR | [SFCN](#CCWld)                               | 64.8  | 107.5 | -  | -  | -  | -           |\n",
    "| 2019--ICCV | [SPN+L2SM](#L2SM)                            | 64.2  | 98.4  | -  | -  | -  | -           |\n",
    "| 2019--CVPR | [TEDnet](#TEDnet)                            | 64.2  | 109.1 | 25.88 | 0.83 | 1.63M | -   |\n",
    "| 2019--CVPR | [ADCrowdNet](#ADCrowdNet)(AMG-bAttn-DME)     | 63.2  | 98.9  | 24.48 | 0.88 | -     | -   |\n",
    "| 2019--CVPR | [PACNN](#PACNN)                              | 66.3  | 106.4 | -  | -  | -  | -           |\n",
    "| 2019--CVPR | [PACNN+CSRNet](#PACNN)                       | 62.4  | 102.0 | -  | -  | -  | -           |\n",
    "| 2019--CVPR | [CAN](#CAN)                                  | 62.3  | 100.0 | -  | -  | -  | -           |\n",
    "| 2019--TIP  |**[HA-CCN](#HA-CCN)**                         | 62.9  | **94.9**| - | - | -  | -           |\n",
    "| 2019--WACV |**[SPN](#SPN)**                               | **61.7** | 99.5 | - | - | -  | -           |\n",
    "\n",
    "#### PartB:\n",
    "| Year-Conference/Journal | Methods                          | MAE   | MSE   |\n",
    "| ---- | ---------------- | ----- | ---- |\n",
    "| 2018--TIP  | [BSAD](#BSAD)                                 | 20.2  | 35.6  |\n",
    "| 2018--WACV | [SaCNN](#SaCNN)                               | 16.2  | 25.8  |\n",
    "| 2018--CVPR | [ACSCP](#ACSCP)                               | 17.2  | 27.4  |\n",
    "| 2018--CVPR | [CSRNet](#CSR)                                | 10.6  | 16.0  |\n",
    "| 2018--CVPR | [IG-CNN](#IG-CNN)                             | 13.6  | 21.1  |\n",
    "| 2018--CVPR | [D-ConvNet-v1](#D-ConvNet)                    | 18.7  | 26.0  |\n",
    "| 2018--CVPR | [DecideNet](#DecideNet)                       | 21.53 | 31.98 |\n",
    "| 2018--CVPR | [DecideNet + R3](#DecideNet)                  | 20.75 | 29.42 |\n",
    "| 2018--CVPR | [L2R](#L2R) (Multi-task,   Query-by-example)  | 14.4  | 23.8  |\n",
    "| 2018--CVPR | [L2R](#L2R) (Multi-task,   Keyword)           | 13.7  | 21.4  |\n",
    "| 2018--IJCAI| [DRSAN](#DRSAN)                               | 11.1  | 18.2  |\n",
    "| 2018--AAAI | [TDF-CNN](#TDF-CNN)                           | 20.7  | 32.8  |\n",
    "| 2018--ECCV | [ic-CNN](#ic-CNN) (one stage)                 | 10.4  | 16.7  |\n",
    "| 2018--ECCV | [ic-CNN](#ic-CNN) (two stages)                | 10.7  | 16.0  |\n",
    "| 2018--ECCV | [SANet](#SANet)                               | 8.4   | 13.6  |\n",
    "| 2019--WACV | [SPN](#SPN)                                   | 9.4   | 14.4  |\n",
    "| 2019--ICASSP | [ASD](#ASD)                                 | 8.5   | 13.7  |\n",
    "| 2019--CVPR | [TEDnet](#TEDnet)                             | 8.2   | 12.8  |\n",
    "| 2019--TIP  | [HA-CCN](#HA-CCN)                             | 8.1   | 13.4  |\n",
    "| 2019--CVPR | [CAN](#CAN)                                   | 7.8   | 12.2  |\n",
    "| 2019--CVPR | [ADCrowdNet](#ADCrowdNet)(AMG-attn-DME)       | 7.7   | 12.9  |\n",
    "| 2019--CVPR | [ADCrowdNet](#ADCrowdNet)(AMG-DME)            | 7.6   | 13.9  |\n",
    "| 2019--CVPR | [SFCN](#CCWld)                                | 7.6   | 13.0  |\n",
    "| 2019--CVPR | [PACNN](#PACNN)                               | 8.9   | 13.5  |\n",
    "| 2019--CVPR | [PACNN+CSRNet](#PACNN)                        | 7.6   | 1.8   |\n",
    "| 2019--ICCV | **[CFF](#CFF)**                               | **7.2** | 12.2 |\n",
    "| 2019--ICCV | **[SPN+L2SM](#L2SM)**                         | **7.2** | **11.1** |\n",
    "\n",
    "\n",
    "### World Expo Dataset:\n",
    "The dataset is splitted into two parts. 1,127 one-minute long video sequences out of 103 scenes are treated as training and validation sets. There are 3 labeled frames in each training video and the interval between two labeled frames is 15 seconds.\n",
    "<img src = 'http://www.ee.cuhk.edu.hk/~xgwang/Project%20Page%20of%20Cross-scene%20Crowd%20Counting%20via%20Deep%20Convolutional%20Neural%20Networks_files/Visio-dataset_sample_2.jpg'>\n",
    "<center><a href = 'http://www.ee.cuhk.edu.hk/~xgwang/expo.html'> Source </a> </center>\n",
    "\n",
    "| Year-Conference/Journal | Method | S1 | S2 | S3 | S4 | S5 | Avg. |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2018--AAAI | [TDF-CNN](#TDF-CNN)                   | 2.7  | 23.4  | 10.7  | 17.6 | 3.3  | 11.5 |\n",
    "| 2018--CVPR | [IG-CNN](#IG-CNN)                     | 2.6  | 16.1  | 10.15 | 20.2 | 7.6  | 11.3 |\n",
    "| 2018--TIP  | [BSAD](#BSAD)                         | 4.1  | 21.7  | 11.9  | 11.0 | 3.5  | 10.5 |\n",
    "| 2018--ECCV | [ic-CNN](#ic-CNN)                     | 17.0 | 12.3  | 9.2   | 8.1  | 4.7  | 10.3 |\n",
    "| 2018--CVPR | [DecideNet](#DecideNet)               | 2.0  | 13.14 | 8.9   | 17.4 | 4.75 | 9.23 |\n",
    "| 2018--CVPR | **[D-ConvNet-v1](#D-ConvNet)**        | 1.9  | 12.1  | 20.7  | 8.3  | **2.6** | 9.1  |\n",
    "| 2018--CVPR | **[CSRNet](#CSR)**                    | 2.9  | 11.5  | **8.6** | 16.6 | 3.4 | 8.6 |\n",
    "| 2018--WACV | [SaCNN](#SaCNN)                       | 2.6  | 13.5  | 10.6  | 12.5 | 3.3  | 8.5  |\n",
    "| 2018--ECCV | [SANet](#SANet)                       | 2.6  | 13.2  | 9.0   | 13.3 | 3.0  | 8.2  |\n",
    "| 2018--IJCAI| [DRSAN](#DRSAN)                       | 2.6  | 11.8  | 10.3  | 10.4 | 3.7  | 7.76 |\n",
    "| 2018--CVPR | [ACSCP](#ACSCP)                       | 2.8  | 14.05 | 9.6   | 8.1  | 2.9  | 7.5  |\n",
    "| 2019--CVPR | **[TEDnet](#TEDnet)**                 | 2.3  | 10.1  | 11.3  | 13.8 | **2.6**  | 8.0  |\n",
    "| 2019--CVPR | [PACNN](#PACNN)                       | 2.3  | 12.5  | 9.1   | 11.2 | 3.8  | 7.8  |\n",
    "| 2019--CVPR | **[ADCrowdNet](#ADCrowdNet)(AMG-bAttn-DME)** | 1.7   | 14.4  | 11.5 | **7.9** | 3.0 | 7.7 |\n",
    "| 2019--CVPR | **[ADCrowdNet](#ADCrowdNet)(AMG-attn-DME)**  | **1.6** | 13.2 | 8.7 | 10.6 | **2.6** | 7.3 |\n",
    "| 2019--CVPR | **[CAN](#CAN)**                       | 2.9  | 12.0  | 10.0  | **7.9** | 4.3 | 7.4  |\n",
    "| 2019--CVPR | **[CAN](#CAN)(ECAN)**                 | 2.4  | **9.4** | 8.8 | 11.2 | 4.0 | **7.2** |\n",
    "\n",
    "\n",
    "\n",
    "### Grand Central Station Dataset:\n",
    "Description:\n",
    "* Length: 33:20 minutes\n",
    "* Frame No.: 50010 frames\n",
    "* Frame Rate: 25fps, 720x480\n",
    "\n",
    "It includes 3 files:\n",
    "\n",
    "* [Video] contains the video compressed into 1.1 GB AVI file by ffmpeg for download convenience.\n",
    "\n",
    "* [Trajectories] contains the KLT keypoint trajectories extractred from the video, which are used in our CVPR2012 paper.\n",
    "\n",
    "* [TrajectoriesNew] contains new bunch of KLT keypoint trajectories extracted from the video with KLT tracker slightly modified.\n",
    "\n",
    "<img src = 'http://www.ee.cuhk.edu.hk/~xgwang/cover.jpg'>\n",
    "<center> Sample Image</center>\n",
    "\n",
    "### UCF  CC 50 Dataset:\n",
    "This data set contains images of extremely dense crowds. The images are collected mainly from the FLICKR. This dataset was used on [Multi-Source Multi-Scale Counting in Extremely Dense Crowd Images](https://www.crcv.ucf.edu/papers/cvpr2013/Counting_V3o.pdf).\n",
    "\n",
    "<img src = 'https://www.crcv.ucf.edu/data/ucf-cc-50/collage.jpg'>\n",
    "<center>Sample image</center>\n",
    "\n",
    "| Year-Conference/Journal | Methods                         | MAE   | MSE   |\n",
    "| ---- | ---------------- | ----- | ---- |\n",
    "| 2018--TIP  | [BSAD](#BSAD)                                | 409.5 | 563.7  |\n",
    "| 2018--AAAI | [TDF-CNN](#TDF-CNN)                          | 354.7 | 491.4  |\n",
    "| 2018--WACV | [SaCNN](#SaCNN)                              | 314.9 | 424.8  |\n",
    "| 2018--CVPR | [IG-CNN](#IG-CNN)                            | 291.4 | 349.4  |\n",
    "| 2018--CVPR | [ACSCP](#ACSCP)                              | 291.0 | 404.6  |\n",
    "| 2018--CVPR | [L2R](#L2R) (Multi-task,   Query-by-example) | 291.5 | 397.6  |\n",
    "| 2018--CVPR | [L2R](#L2R) (Multi-task,   Keyword)          | 279.6 | 388.9  |\n",
    "| 2018--CVPR | [D-ConvNet-v1](#D-ConvNet)                   | 288.4 | 404.7  |\n",
    "| 2018--CVPR | [CSRNet](#CSR)                               | 266.1 | 397.5  |\n",
    "| 2018--ECCV | [ic-CNN](#ic-CNN) (two stages)               | 260.9 | 365.5  |\n",
    "| 2018--ECCV | [SANet](#SANet)                              | 258.4 | 334.9  |\n",
    "| 2018--IJCAI| [DRSAN](#DRSAN)                              | 219.2 | 250.2  |\n",
    "| 2019--AAAI | [GWTA-CCNN](#GWTA-CCNN)                      | 433.7 | 583.3  |\n",
    "| 2019--WACV | [SPN](#SPN)                                  | 259.2 | 335.9  |\n",
    "| 2019--CVPR | [ADCrowdNet](#ADCrowdNet)(DME)               | 257.1 | 363.5  |\n",
    "| 2019--TIP  | [HA-CCN](#HA-CCN)                            | 256.2 | 348.4  |\n",
    "| 2019--CVPR | [TEDnet](#TEDnet)                            | 249.4 | 354.5  |\n",
    "| 2019--CVPR | [PACNN](#PACNN)                              | 267.9 | 357.8  |\n",
    "| 2019--CVPR | [PACNN+CSRNet](#PACNN)                       | 241.7 | 320.7  |\n",
    "| 2019--CVPR | [SFCN](#CCWld)                               | 214.2 | 318.2  |\n",
    "| 2019--CVPR | **[CAN](#CAN)**                              | 212.2 | **243.7** |\n",
    "| 2019--ICASSP| [ASD](#ASD)                                 | 196.2 | 270.9  |\n",
    "| 2019--ICCV | **[SPN+L2SM](#L2SM)**                        | **188.4** | 315.3 |\n",
    "\n",
    "\n",
    "\n",
    "### UCF-QNRF - A Large Crowd Counting Data Set:\n",
    "It contains 1535 images which are divided into train and test sets of 1201 and 334 images respectively. According to authors, this dataset is most suitable for training very deep Convolutional Neural Networks (CNNs) since it contains order of magnitude more annotated humans in dense crowd scenes than any other available crowd counting dataset. Authors have even provided the comparison of dataset with other dataset. This dataset was used on [Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds](https://www.crcv.ucf.edu/papers/eccv2018/2324.pdf).\n",
    "<img src = 'https://www.crcv.ucf.edu/data/ucf-qnrf/ucf-qnrf-sample.png'>\n",
    "<center>Sample image</center>\n",
    "\n",
    "### UCF-QNRF\n",
    "\n",
    "| Year-Conference/Journal | Method | C-MAE | C-NAE | C-MSE | DM-MAE | DM-MSE | DM-HI | L- Av. Precision\t| L-Av. Recall | L-AUC |\n",
    "| --- | --- | --- | --- |--- | --- | --- |--- | --- | --- | ---|\n",
    "| 2018--ECCV | [CL](#CL)     | 132 | 0.26 | 191 | 0.00044| 0.0017 | 0.9131 | 75.8% | 59.75%\t| 0.714|\n",
    "| 2019--TIP  | [HA-CCN](#HA-CCN)   | 118.1 | - | 180.4 | - | - | - | - | - | - |\n",
    "| 2019--CVPR | [TEDnet](#TEDnet)   | 113 | - | 188 | - | - | - | - | - | - |\n",
    "| 2019--CVPR | [CAN](#CAN)   | 107 | - | 183 | - | - | - | - | - | - |\n",
    "| 2019--ICCV | [SPN+L2SM](#L2SM)   | 104.7 | - | 173.6 | - | - | - | - | - | - |\n",
    "| 2019--CVPR | **[SFCN](#CCWld)**  | **102.0** | - | **171.4** | - | - | - | - | - | - |\n",
    "\n",
    "\n",
    "## Papers:\n",
    "### 2019\n",
    "- <a name=\"CFF\"></a> **[CFF]** Counting with Focus for Free (**ICCV2019**) [[paper](https://arxiv.org/abs/1903.12206)][[code](https://github.com/shizenglin/Counting-with-Focus-for-Free)] \n",
    "- <a name=\"L2SM\"></a> **[L2SM]** Learn to Scale: Generating Multipolar Normalized Density Map for Crowd Counting (**ICCV2019**) [[paper](https://arxiv.org/abs/1907.12428)]\n",
    "- <a name=\"\"></a> Relational Attention Network for Crowd Counting (**ICCV2019**)\n",
    "- <a name=\"\"></a> Attentional Neural Fields for Crowd Counting (**ICCV2019**)\n",
    "- <a name=\"RAZ-Net\"></a> **[RAZ-Net]** Recurrent Attentive Zooming for Joint Crowd Counting and Precise Localization (**CVPR2019**) [[paper](http://www.muyadong.com/paper/cvpr19_0484.pdf)]\n",
    "- <a name=\"RDNet\"></a> **[RDNet]** Density Map Regression Guided Detection Network for RGB-D Crowd Counting and Localization (**CVPR2019**) [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Lian_Density_Map_Regression_Guided_Detection_Network_for_RGB-D_Crowd_Counting_CVPR_2019_paper.pdf)]\n",
    "- <a name=\"RRSP\"></a> **[RRSP]** Residual Regression with Semantic Prior for Crowd Counting (**CVPR2019**) [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wan_Residual_Regression_With_Semantic_Prior_for_Crowd_Counting_CVPR_2019_paper.pdf)]\n",
    "- <a name=\"MVMS\"></a> **[MVMS]** Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs (**CVPR2019**) [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Wide-Area_Crowd_Counting_via_Ground-Plane_Density_Maps_and_Multi-View_Fusion_CVPR_2019_paper.pdf)]\n",
    "- <a name=\"AT-CFCN\"></a> **[AT-CFCN]** Leveraging Heterogeneous Auxiliary Tasks to Assist Crowd Counting (**CVPR2019**) [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Leveraging_Heterogeneous_Auxiliary_Tasks_to_Assist_Crowd_Counting_CVPR_2019_paper.pdf)]\n",
    "- <a name=\"TEDnet\"></a> **[TEDnet]** Crowd Counting and Density Estimation by Trellis Encoder-Decoder Networks (**CVPR2019**) [[paper](https://arxiv.org/abs/1903.00853)]\n",
    "- <a name=\"CAN\"></a> **[CAN]** Context-Aware Crowd Counting (**CVPR2019**) [[paper](https://arxiv.org/pdf/1811.10452.pdf)] [official code: [PyTorch](https://github.com/weizheliu/Context-Aware-Crowd-Counting)]\n",
    "- <a name=\"PACNN\"></a> **[PACNN]** Revisiting Perspective Information for Efficient Crowd Counting (**CVPR2019**)[[paper](https://arxiv.org/abs/1807.01989v3)]\n",
    "- <a name=\"PSDDN\"></a> **[PSDDN]** Point in, Box out: Beyond Counting Persons in Crowds (**CVPR2019**)[[paper](https://arxiv.org/abs/1904.01333)]\n",
    "- <a name=\"ADCrowdNet\"></a> **[ADCrowdNet]** ADCrowdNet: An Attention-injective Deformable Convolutional Network for Crowd Understanding (**CVPR2019**) [[paper](https://arxiv.org/abs/1811.11968)]\n",
    "- <a name=\"CCWld\"></a> **[CCWld, SFCN]** Learning from Synthetic Data for Crowd Counting in the Wild (**CVPR2019**) [[paper](http://gjy3035.github.io/pdf/CC_Wild_0308_cvpr2019.pdf)] [[Project](https://gjy3035.github.io/GCC-CL/)] [[arxiv](https://arxiv.org/abs/1903.03303)]\n",
    "- <a name=\"SL2R\"></a>  **[SL2R]** Exploiting Unlabeled Data in CNNs by Self-supervised Learning to Rank (**T-PAMI**) [[paper](https://arxiv.org/abs/1902.06285)](extension of [L2R](#L2R))\n",
    "- <a name=\"IA-DNN\"></a> **[IA-DNN]** Inverse Attention Guided Deep Crowd Counting Network (**AVSS2019**) [[paper](https://arxiv.org/pdf/1907.01193.pdf)]\n",
    "- <a name=\"CODA\"></a> **[CODA]** CODA: Counting Objects via Scale-aware Adversarial Density Adaption (**ICME2019**) [[paper](https://arxiv.org/abs/1903.10442)][[code](https://github.com/Willy0919/CODA)]\n",
    "- <a name=\"LSTN\"></a> **[LSTN]** Locality-Constrained Spatial Transformer Network for Video Crowd Counting (**ICME2019**)  [[paper](https://arxiv.org/abs/1907.07911)]\n",
    "- <a name=\"\"></a> Dynamic Region Division for Adaptive Learning Pedestrian Counting (**ICME2019**) \n",
    "- <a name=\"\"></a> Crowd Counting via Multi-View Scale Aggregation Networks (**ICME2019**)\n",
    "- <a name=\"ASD\"></a> **[ASD]** Adaptive Scenario Discovery for Crowd Counting (**ICASSP2019**) [[paper](https://arxiv.org/pdf/1812.02393.pdf)]\n",
    "- <a name=\"SAAN\"></a> **[SAAN]** Crowd Counting Using Scale-Aware Attention Networks (**WACV2019**) [[paper](http://www.cs.umanitoba.ca/~ywang/papers/wacv19.pdf)]\n",
    "- <a name=\"SPN\"></a> **[SPN]** Scale Pyramid Network for Crowd Counting (**WACV2019**) [[paper](http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=8642793)]\n",
    "- <a name=\"GWTA-CCNN\"></a> **[GWTA-CCNN]** Almost Unsupervised Learning for Dense Crowd Counting (**AAAI2019**) [[paper](http://val.serc.iisc.ernet.in/valweb/papers/AAAI_2019_WTACNN.pdf)]\n",
    "- <a name=\"PCC-Net\"></a> **[PCC-Net]** PCC Net: Perspective Crowd Counting via Spatial Convolutional Network (**T-CSVT2019**) [[paper](https://arxiv.org/abs/1905.10085)] [[code](https://github.com/gjy3035/PCC-Net)]\n",
    "- <a name=\"CLPC\"></a> **[CLPC]** Cross-Line Pedestrian Counting Based on Spatially-Consistent Two-Stage Local Crowd Density Estimation and Accumulation (**T-CSVT2019**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8295124)]\n",
    "- <a name=\"CCLL\"></a> **[CCLL]** Crowd Counting With Limited Labeling Through Submodular Frame Selection (**T-CSVT2019**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8360780)]\n",
    "- <a name=\"ACSPNet\"></a> **[ACSPNet]** Atrous convolutions spatial pyramid network for crowd counting and density estimation (**Neurocomputing2019**) [[paper](https://www.sciencedirect.com/science/article/pii/S0925231219304059)]\n",
    "- <a name=\"DDCN\"></a> **[DDCN]** Removing background interference for crowd counting via de-background detail convolutional network (**Neurocomputing2019**) [[paper](https://www.sciencedirect.com/science/article/pii/S0925231218315042)]\n",
    "- <a name=\"MRA-CNN\"></a> **[MRA-CNN]** Multi-resolution attention convolutional neural network for crowd counting (**Neurocomputing2019**) [[paper](https://www.sciencedirect.com/science/article/pii/S0925231218312542)]\n",
    "- <a name=\"HA-CCN\"></a> **[HA-CCN]** HA-CCN: Hierarchical Attention-based Crowd Counting Network (**TIP2019**) [[paper](https://arxiv.org/abs/1907.10255)]\n",
    "- <a name=\"\"></a> Geometric and Physical Constraints for Drone-Based Head Plane Crowd Density Estimation (**IROS2019**) [[paper](https://arxiv.org/abs/1803.08805)]\n",
    "\n",
    "### 2018\n",
    "- <a name=\"LCFCN\"></a> **[LCFCN]**  Where are the Blobs: Counting by Localization with Point Supervision (**ECCV2018**) [[paper](https://arxiv.org/abs/1807.09856)] [[code](https://github.com/ElementAI/LCFCN)]\n",
    "- <a name=\"CAC\"></a>**[CAC]** Class-Agnostic Counting (**ACCV2018**) [[paper](https://arxiv.org/abs/1811.00472)] [[code](https://github.com/erikalu/class-agnostic-counting)]\n",
    "- <a name=\"SCNet\"></a>**[SCNet]** In Defense of Single-column Networks for Crowd Counting (**BMVC2018**) [[paper](https://arxiv.org/abs/1808.06133)]\n",
    "- <a name=\"AFP\"></a>**[AFP]** Crowd Counting by Adaptively Fusing Predictions from an Image Pyramid (**BMVC2018**) [[paper](https://arxiv.org/abs/1805.06115)]\n",
    "- <a name=\"DRSAN\"></a>**[DRSAN]** Crowd Counting using Deep Recurrent Spatial-Aware Network (**IJCAI2018**) [[paper](https://arxiv.org/abs/1807.00601)]\n",
    "- <a name=\"TDF-CNN\"></a>**[TDF-CNN]** Top-Down Feedback for Crowd Counting Convolutional Neural Network (**AAAI2018**) [[paper](https://arxiv.org/abs/1807.08881)]\n",
    "- <a name=\"SANet\"></a> **[SANet]** Scale Aggregation Network for Accurate and Efficient Crowd Counting (**ECCV2018**) [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/Xinkun_Cao_Scale_Aggregation_Network_ECCV_2018_paper.pdf)]\n",
    "- <a name=\"ic-CNN\"></a> **[ic-CNN]** Iterative Crowd Counting (**ECCV2018**) [[paper](https://arxiv.org/abs/1807.09959)]\n",
    "- <a name=\"CL\"></a> **[CL]** Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds (**ECCV2018**) [[paper](https://arxiv.org/abs/1808.01050)]\n",
    "- <a name=\"D-ConvNet\"></a> **[D-ConvNet]** Crowd Counting with Deep Negative Correlation Learning (**CVPR2018**) [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf)] [[code](https://github.com/shizenglin/Deep-NCL)]\n",
    "- <a name=\"IG-CNN\"></a> **[IG-CNN]** Divide and Grow: Capturing Huge Diversity in Crowd Images with\n",
    "Incrementally Growing CNN (**CVPR2018**) [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sam_Divide_and_Grow_CVPR_2018_paper.pdf)]\n",
    "- <a name=\"BSAD\"></a> **[BSAD]** Body Structure Aware Deep Crowd Counting (**TIP2018**) [[paper](http://mac.xmu.edu.cn/rrji/papers/IP%202018-Body.pdf)] \n",
    "- <a name=\"CSR\"></a> **[CSR]**  CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes (**CVPR2018**) [[paper](https://arxiv.org/abs/1802.10062)] [[code](https://github.com/leeyeehoo/CSRNet-pytorch)]\n",
    "- <a name=\"L2R\"></a>  **[L2R]** Leveraging Unlabeled Data for Crowd Counting by Learning to Rank (**CVPR2018**) [[paper](https://arxiv.org/abs/1803.03095)] [[code](https://github.com/xialeiliu/CrowdCountingCVPR18)] \n",
    "- <a name=\"ACSCP\"></a> **[ACSCP]**  Crowd Counting via Adversarial Cross-Scale Consistency Pursuit  (**CVPR2018**) [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Crowd_Counting_via_CVPR_2018_paper.pdf)]   [unofficial code: [PyTorch](https://github.com/RQuispeC/pytorch-ACSCP)]\n",
    "- <a name=\"DecideNet\"></a> **[DecideNet]** DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density (**CVPR2018**) [[paper](https://arxiv.org/abs/1712.06679)]\n",
    "- <a name=\"AMDCN\"></a>  **[AMDCN]** An Aggregated Multicolumn Dilated Convolution Network for Perspective-Free Counting (**CVPR2018**) [[paper](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w6/Deb_An_Aggregated_Multicolumn_CVPR_2018_paper.pdf)] [[code](https://github.com/diptodip/counting)] \n",
    "- <a name=\"A-CCNN\"></a> **[A-CCNN]** A-CCNN: Adaptive CCNN for Density Estimation and Crowd Counting (**ICIP2018**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8451399)]\n",
    "- <a name=\"\"></a> Crowd Counting with Fully Convolutional Neural Network (**ICIP2018**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8451787)]\n",
    "- <a name=\"MS-GAN\"></a> **[MS-GAN]** Multi-scale Generative Adversarial Networks for Crowd Counting (**ICPR2018**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8545683)]\n",
    "- <a name=\"DR-ResNet\"></a> **[DR-ResNet]** A Deeply-Recursive Convolutional Network for Crowd Counting (**ICASSP2018**) [[paper](https://arxiv.org/abs/1805.05633)] \n",
    "- <a name=\"GAN-MTR\"></a> **[GAN-MTR]** Crowd Counting With Minimal Data Using Generative Adversarial Networks For Multiple Target Regression (**WACV2018**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8354235)]\n",
    "- <a name=\"SaCNN\"></a> **[SaCNN]** Crowd counting via scale-adaptive convolutional neural network (**WACV2018**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8354231)] [[code](https://github.com/miao0913/SaCNN-CrowdCounting-Tencent_Youtu)]\n",
    "- <a name=\"Improved SaCNN\"></a> **[Improved SaCNN]** Improved Crowd Counting Method Based on Scale-Adaptive Convolutional Neural Network (**IEEE Access**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8643345)]\n",
    "- <a name=\"DA-Net\"></a> **[DA-Net]** DA-Net: Learning the Fine-Grained Density Distribution With Deformation Aggregation Network (**IEEE Access**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8497050)][[code](https://github.com/BigTeacher-777/DA-Net)]\n",
    "- <a name=\"NetVLAD\"></a> **[NetVLAD]** Multiscale Multitask Deep NetVLAD for Crowd Counting (**TII2018**) [[paper](https://staff.fnwi.uva.nl/z.shi/files/counting-netvlad.pdf)] [[code](https://github.com/shizenglin/Multitask-Multiscale-Deep-NetVLAD)]\n",
    "- <a name=\"W-VLAD\"></a> **[W-VLAD]** Crowd Counting via Weighted VLAD on Dense Attribute Feature Maps (**T-CSVT2018**) [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7778134)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Useful Links\n",
    "\n",
    "* [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/02/building-crowd-counting-model-python/)\n",
    "* [Learning To Count Objects in Images](http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images)\n",
    "* [Algorithm Spotlight: Crowd Counter](https://blog.algorithmia.com/algorithm-spotlight-crowd-counter/)\n",
    "\n",
    "## Useful Papers\n",
    "| Name | | Year Published |\n",
    "| --- | --- |\n",
    "| [Single-Image Crowd Counting via Multi-Column Convolutional Neural Network](https://ieeexplore.ieee.org/document/7780439) | 2016 |\n",
    "| [Recent survey on crowd density estimation and counting for visual surveillance](https://www.academia.edu/28188132/Recent_survey_on_crowd_density_estimation_and_counting_for_visual_surveillance)| 2015|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
